{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Study Analysis\n",
    "\n",
    "This notebook analyzes the results from the simulation study comparing the Bellman Filter and Particle Filter implementations for the Dynamic Factor Stochastic Volatility (DFSV) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 24 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   N               99 non-null     int64  \n",
      " 1   K               99 non-null     int64  \n",
      " 2   T               99 non-null     int64  \n",
      " 3   num_particles   66 non-null     float64\n",
      " 4   seed            99 non-null     int64  \n",
      " 5   bf_time         33 non-null     float64\n",
      " 6   pf_time         66 non-null     float64\n",
      " 7   bf_rmse_f       33 non-null     object \n",
      " 8   bf_corr_f       33 non-null     object \n",
      " 9   bf_rmse_h       33 non-null     object \n",
      " 10  bf_corr_h       33 non-null     object \n",
      " 11  pf_rmse_f       66 non-null     object \n",
      " 12  pf_corr_f       66 non-null     object \n",
      " 13  pf_rmse_h       66 non-null     object \n",
      " 14  pf_corr_h       66 non-null     object \n",
      " 15  error           0 non-null      float64\n",
      " 16  bf_rmse_f_mean  33 non-null     float64\n",
      " 17  bf_rmse_h_mean  33 non-null     float64\n",
      " 18  bf_corr_f_mean  33 non-null     float64\n",
      " 19  bf_corr_h_mean  27 non-null     float64\n",
      " 20  pf_rmse_f_mean  66 non-null     float64\n",
      " 21  pf_rmse_h_mean  66 non-null     float64\n",
      " 22  pf_corr_f_mean  66 non-null     float64\n",
      " 23  pf_corr_h_mean  66 non-null     float64\n",
      "dtypes: float64(12), int64(4), object(8)\n",
      "memory usage: 18.7+ KB\n",
      "None\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>K</th>\n",
       "      <th>T</th>\n",
       "      <th>num_particles</th>\n",
       "      <th>seed</th>\n",
       "      <th>bf_time</th>\n",
       "      <th>pf_time</th>\n",
       "      <th>bf_rmse_f</th>\n",
       "      <th>bf_corr_f</th>\n",
       "      <th>bf_rmse_h</th>\n",
       "      <th>...</th>\n",
       "      <th>pf_corr_h</th>\n",
       "      <th>error</th>\n",
       "      <th>bf_rmse_f_mean</th>\n",
       "      <th>bf_rmse_h_mean</th>\n",
       "      <th>bf_corr_f_mean</th>\n",
       "      <th>bf_corr_h_mean</th>\n",
       "      <th>pf_rmse_f_mean</th>\n",
       "      <th>pf_rmse_h_mean</th>\n",
       "      <th>pf_corr_f_mean</th>\n",
       "      <th>pf_corr_h_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5200</td>\n",
       "      <td>0.650460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.47707086 0.61443548]</td>\n",
       "      <td>[0.94311204 0.88114631]</td>\n",
       "      <td>[0.09325253 0.12443127]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.545753</td>\n",
       "      <td>0.108842</td>\n",
       "      <td>0.912129</td>\n",
       "      <td>-3.538686e-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5201</td>\n",
       "      <td>0.507786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.39551383 0.40863758]</td>\n",
       "      <td>[0.95802971 0.89678362]</td>\n",
       "      <td>[0.15845739 0.08927448]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.402076</td>\n",
       "      <td>0.123866</td>\n",
       "      <td>0.927407</td>\n",
       "      <td>2.063489e-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5202</td>\n",
       "      <td>0.466367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.38785858 0.46385911]</td>\n",
       "      <td>[0.93809231 0.96356005]</td>\n",
       "      <td>[0.1926684 0.2231097]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.425859</td>\n",
       "      <td>0.207889</td>\n",
       "      <td>0.950826</td>\n",
       "      <td>-1.895160e-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>6200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.918666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.11121595 0.10232044]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665355</td>\n",
       "      <td>0.350185</td>\n",
       "      <td>0.570232</td>\n",
       "      <td>0.106768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>6201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.650595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[0.06545748 0.06487638]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.665151</td>\n",
       "      <td>0.224602</td>\n",
       "      <td>0.479633</td>\n",
       "      <td>0.065167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   N  K     T  num_particles  seed   bf_time   pf_time  \\\n",
       "0  5  2  1000            NaN  5200  0.650460       NaN   \n",
       "1  5  2  1000            NaN  5201  0.507786       NaN   \n",
       "2  5  2  1000            NaN  5202  0.466367       NaN   \n",
       "3  5  2  1000         1000.0  6200       NaN  0.918666   \n",
       "4  5  2  1000         1000.0  6201       NaN  0.650595   \n",
       "\n",
       "                 bf_rmse_f                bf_corr_f                bf_rmse_h  \\\n",
       "0  [0.47707086 0.61443548]  [0.94311204 0.88114631]  [0.09325253 0.12443127]   \n",
       "1  [0.39551383 0.40863758]  [0.95802971 0.89678362]  [0.15845739 0.08927448]   \n",
       "2  [0.38785858 0.46385911]  [0.93809231 0.96356005]    [0.1926684 0.2231097]   \n",
       "3                      NaN                      NaN                      NaN   \n",
       "4                      NaN                      NaN                      NaN   \n",
       "\n",
       "   ...                pf_corr_h error bf_rmse_f_mean bf_rmse_h_mean  \\\n",
       "0  ...                      NaN   NaN       0.545753       0.108842   \n",
       "1  ...                      NaN   NaN       0.402076       0.123866   \n",
       "2  ...                      NaN   NaN       0.425859       0.207889   \n",
       "3  ...  [0.11121595 0.10232044]   NaN            NaN            NaN   \n",
       "4  ...  [0.06545748 0.06487638]   NaN            NaN            NaN   \n",
       "\n",
       "  bf_corr_f_mean  bf_corr_h_mean  pf_rmse_f_mean  pf_rmse_h_mean  \\\n",
       "0       0.912129   -3.538686e-17             NaN             NaN   \n",
       "1       0.927407    2.063489e-16             NaN             NaN   \n",
       "2       0.950826   -1.895160e-16             NaN             NaN   \n",
       "3            NaN             NaN        0.665355        0.350185   \n",
       "4            NaN             NaN        0.665151        0.224602   \n",
       "\n",
       "   pf_corr_f_mean  pf_corr_h_mean  \n",
       "0             NaN             NaN  \n",
       "1             NaN             NaN  \n",
       "2             NaN             NaN  \n",
       "3        0.570232        0.106768  \n",
       "4        0.479633        0.065167  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the default template to a clean, modern style\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Read the simulation results\n",
    "results_df = pd.read_csv('simulation_results.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(results_df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's clean and prepare the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[2], line 6\u001b[0m\n    results_df[col] = results_df[col].apply(lambda x: np.array(eval(x)) if isinstance(x, str) else x)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/pandas/core/series.py:4924\u001b[0m in \u001b[1;35mapply\u001b[0m\n    ).apply()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001b[0m in \u001b[1;35mapply\u001b[0m\n    return self.apply_standard()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001b[0m in \u001b[1;35mapply_standard\u001b[0m\n    mapped = obj._map_values(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/pandas/core/base.py:921\u001b[0m in \u001b[1;35m_map_values\u001b[0m\n    return algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniforge3/envs/Main_thesis/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[0m in \u001b[1;35mmap_array\u001b[0m\n    return lib.map_infer(values, mapper, convert=convert)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32mlib.pyx:2972\u001b[0m in \u001b[1;35mpandas._libs.lib.map_infer\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 6\u001b[0;36m in \u001b[0;35m<lambda>\u001b[0;36m\n\u001b[0;31m    results_df[col] = results_df[col].apply(lambda x: np.array(eval(x)) if isinstance(x, str) else x)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [0.47707086 0.61443548]\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Convert array columns to proper format\n",
    "array_columns = ['bf_rmse_f', 'bf_corr_f', 'bf_rmse_h', 'bf_corr_h',\n",
    "                'pf_rmse_f', 'pf_corr_f', 'pf_rmse_h', 'pf_corr_h']\n",
    "\n",
    "for col in array_columns:\n",
    "    results_df[col] = results_df[col].apply(lambda x: np.array(eval(x)) if isinstance(x, str) else x)\n",
    "\n",
    "# Calculate mean values across factors/volatilities\n",
    "for filt in ['bf', 'pf']:\n",
    "    for metric in ['rmse', 'corr']:\n",
    "        for state in ['f', 'h']:\n",
    "            col_name = f'{filt}_{metric}_{state}'\n",
    "            results_df[f'{col_name}_mean'] = results_df[col_name].apply(lambda x: np.mean(x) if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "# Aggregate results across replications\n",
    "agg_results = results_df.groupby(['N', 'K', 'num_particles']).agg({\n",
    "    'bf_time': 'mean',\n",
    "    'pf_time': 'mean',\n",
    "    'bf_corr_f_mean': 'mean',\n",
    "    'pf_corr_f_mean': 'mean',\n",
    "    'bf_corr_h_mean': 'mean',\n",
    "    'pf_corr_h_mean': 'mean',\n",
    "    'bf_rmse_f_mean': 'mean',\n",
    "    'pf_rmse_f_mean': 'mean',\n",
    "    'bf_rmse_h_mean': 'mean',\n",
    "    'pf_rmse_h_mean': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Aggregated Results:\")\n",
    "display(agg_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance of both filters across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for different performance metrics\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Computation Time vs K',\n",
    "        'Factor Estimation Accuracy',\n",
    "        'Log-Volatility Estimation Accuracy',\n",
    "        'Computation Time vs N'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Time vs K for different N\n",
    "for n_val in agg_results['N'].unique():\n",
    "    # Bellman Filter\n",
    "    bf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'].isna())]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bf_subset['K'],\n",
    "            y=bf_subset['bf_time'],\n",
    "            name=f'BF (N={n_val})',\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=8)\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Particle Filter with different particle counts\n",
    "    for num_particles in [1000, 10000]:\n",
    "        pf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'] == num_particles)]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pf_subset['K'],\n",
    "                y=pf_subset['pf_time'],\n",
    "                name=f'PF (N={n_val}, {num_particles} particles)',\n",
    "                mode='lines+markers',\n",
    "                line=dict(width=2, dash='dash'),\n",
    "                marker=dict(size=8)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "# Factor Correlation vs K\n",
    "for n_val in agg_results['N'].unique():\n",
    "    # Bellman Filter\n",
    "    bf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'].isna())]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bf_subset['K'],\n",
    "            y=bf_subset['bf_corr_f_mean'],\n",
    "            name=f'BF (N={n_val})',\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    # Particle Filter with different particle counts\n",
    "    for num_particles in [1000, 10000]:\n",
    "        pf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'] == num_particles)]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pf_subset['K'],\n",
    "                y=pf_subset['pf_corr_f_mean'],\n",
    "                name=f'PF (N={n_val}, {num_particles} particles)',\n",
    "                mode='lines+markers',\n",
    "                line=dict(width=2, dash='dash'),\n",
    "                marker=dict(size=8),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "# Log-Volatility Correlation vs K\n",
    "for n_val in agg_results['N'].unique():\n",
    "    # Bellman Filter\n",
    "    bf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'].isna())]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bf_subset['K'],\n",
    "            y=bf_subset['bf_corr_h_mean'],\n",
    "            name=f'BF (N={n_val})',\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Particle Filter with different particle counts\n",
    "    for num_particles in [1000, 10000]:\n",
    "        pf_subset = agg_results[(agg_results['N'] == n_val) & (agg_results['num_particles'] == num_particles)]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pf_subset['K'],\n",
    "                y=pf_subset['pf_corr_h_mean'],\n",
    "                name=f'PF (N={n_val}, {num_particles} particles)',\n",
    "                mode='lines+markers',\n",
    "                line=dict(width=2, dash='dash'),\n",
    "                marker=dict(size=8),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "# Time vs N for different K\n",
    "for k_val in agg_results['K'].unique():\n",
    "    # Bellman Filter\n",
    "    bf_subset = agg_results[(agg_results['K'] == k_val) & (agg_results['num_particles'].isna())]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=bf_subset['N'],\n",
    "            y=bf_subset['bf_time'],\n",
    "            name=f'BF (K={k_val})',\n",
    "            mode='lines+markers',\n",
    "            line=dict(width=2),\n",
    "            marker=dict(size=8),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "    # Particle Filter with different particle counts\n",
    "    for num_particles in [1000, 10000]:\n",
    "        pf_subset = agg_results[(agg_results['K'] == k_val) & (agg_results['num_particles'] == num_particles)]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=pf_subset['N'],\n",
    "                y=pf_subset['pf_time'],\n",
    "                name=f'PF (K={k_val}, {num_particles} particles)',\n",
    "                mode='lines+markers',\n",
    "                line=dict(width=2, dash='dash'),\n",
    "                marker=dict(size=8),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    width=1200,\n",
    "    title_text=\"Simulation Study Results (Averaged over Replications)\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=1.05\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_xaxes(title_text=\"K (Number of Factors)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"K (Number of Factors)\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"K (Number of Factors)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"N (Number of Assets)\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"Average Computation Time (s)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Factor Correlation\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Average Log-Volatility Correlation\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Computation Time (s)\", row=2, col=2)\n",
    "\n",
    "# Set y-axis ranges for correlation plots\n",
    "fig.update_yaxes(range=[0, 1], row=1, col=2)\n",
    "fig.update_yaxes(range=[0, 1], row=2, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis\n",
    "\n",
    "Let's perform some statistical analysis to compare the performance of the filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for each filter\n",
    "print(\"Summary Statistics for Bellman Filter:\")\n",
    "bf_stats = agg_results[agg_results['num_particles'].isna()].agg({\n",
    "    'bf_time': ['mean', 'std'],\n",
    "    'bf_corr_f_mean': ['mean', 'std'],\n",
    "    'bf_corr_h_mean': ['mean', 'std'],\n",
    "    'bf_rmse_f_mean': ['mean', 'std'],\n",
    "    'bf_rmse_h_mean': ['mean', 'std']\n",
    "})\n",
    "display(bf_stats)\n",
    "\n",
    "print(\"\\nSummary Statistics for Particle Filter (1000 particles):\")\n",
    "pf_1000_stats = agg_results[agg_results['num_particles'] == 1000].agg({\n",
    "    'pf_time': ['mean', 'std'],\n",
    "    'pf_corr_f_mean': ['mean', 'std'],\n",
    "    'pf_corr_h_mean': ['mean', 'std'],\n",
    "    'pf_rmse_f_mean': ['mean', 'std'],\n",
    "    'pf_rmse_h_mean': ['mean', 'std']\n",
    "})\n",
    "display(pf_1000_stats)\n",
    "\n",
    "print(\"\\nSummary Statistics for Particle Filter (10000 particles):\")\n",
    "pf_10000_stats = agg_results[agg_results['num_particles'] == 10000].agg({\n",
    "    'pf_time': ['mean', 'std'],\n",
    "    'pf_corr_f_mean': ['mean', 'std'],\n",
    "    'pf_corr_h_mean': ['mean', 'std'],\n",
    "    'pf_rmse_f_mean': ['mean', 'std'],\n",
    "    'pf_rmse_h_mean': ['mean', 'std']\n",
    "})\n",
    "display(pf_10000_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison by Configuration\n",
    "\n",
    "Let's analyze how the performance varies with different configurations of N and K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of computation times for different N and K combinations\n",
    "def create_heatmap(data, metric, title):\n",
    "    pivot_data = data.pivot(index='N', columns='K', values=metric)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=pivot_data.values,\n",
    "        x=pivot_data.columns,\n",
    "        y=pivot_data.index,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title=metric)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='K (Number of Factors)',\n",
    "        yaxis_title='N (Number of Assets)',\n",
    "        height=500,\n",
    "        width=700\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create heatmaps for different metrics\n",
    "bf_data = agg_results[agg_results['num_particles'].isna()]\n",
    "pf_1000_data = agg_results[agg_results['num_particles'] == 1000]\n",
    "pf_10000_data = agg_results[agg_results['num_particles'] == 10000]\n",
    "\n",
    "# Bellman Filter heatmaps\n",
    "fig_bf_time = create_heatmap(bf_data, 'bf_time', 'Bellman Filter Computation Time')\n",
    "fig_bf_corr = create_heatmap(bf_data, 'bf_corr_f_mean', 'Bellman Filter Factor Correlation')\n",
    "\n",
    "# Particle Filter (1000 particles) heatmaps\n",
    "fig_pf1000_time = create_heatmap(pf_1000_data, 'pf_time', 'Particle Filter (1000 particles) Computation Time')\n",
    "fig_pf1000_corr = create_heatmap(pf_1000_data, 'pf_corr_f_mean', 'Particle Filter (1000 particles) Factor Correlation')\n",
    "\n",
    "# Particle Filter (10000 particles) heatmaps\n",
    "fig_pf10000_time = create_heatmap(pf_10000_data, 'pf_time', 'Particle Filter (10000 particles) Computation Time')\n",
    "fig_pf10000_corr = create_heatmap(pf_10000_data, 'pf_corr_f_mean', 'Particle Filter (10000 particles) Factor Correlation')\n",
    "\n",
    "# Display the heatmaps\n",
    "fig_bf_time.show()\n",
    "fig_bf_corr.show()\n",
    "fig_pf1000_time.show()\n",
    "fig_pf1000_corr.show()\n",
    "fig_pf10000_time.show()\n",
    "fig_pf10000_corr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Based on the analysis above, we can draw several conclusions:\n",
    "\n",
    "1. **Computation Time**:\n",
    "   - The Bellman Filter generally shows more consistent computation times across different configurations\n",
    "   - The Particle Filter's computation time increases significantly with the number of particles\n",
    "   - Both filters show increasing computation time with larger N and K values\n",
    "\n",
    "2. **Estimation Accuracy**:\n",
    "   - The Bellman Filter shows high correlation for factor estimation across most configurations\n",
    "   - The Particle Filter's accuracy improves with more particles but at the cost of computation time\n",
    "   - Both filters show better performance for smaller values of K\n",
    "\n",
    "3. **Scalability**:\n",
    "   - The Bellman Filter shows better scalability with respect to N and K\n",
    "   - The Particle Filter's performance degrades more rapidly with increasing N and K\n",
    "   - The trade-off between accuracy and computation time is more pronounced for the Particle Filter\n",
    "\n",
    "4. **Overall Performance**:\n",
    "   - The Bellman Filter offers a good balance between accuracy and computation time\n",
    "   - The Particle Filter with 10000 particles can achieve better accuracy but at a significant computational cost\n",
    "   - The choice between filters depends on the specific requirements for accuracy vs. computation time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Main_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
