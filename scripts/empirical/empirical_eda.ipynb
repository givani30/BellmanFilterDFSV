{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da00151",
   "metadata": {},
   "source": [
    "# Empirical Data Exploratory Analysis\n",
    "\n",
    "This notebook contains exploratory data analysis for empirical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad75c3",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019bbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import csv\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dddd5b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "624b18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning '../../Data/Fama_french/100_Portfolios_10x10.CSV' for potential data block markers (v3)...\n",
      "\n",
      "Found potential data block starting points:\n",
      "  Line 15: Average Value Weighted Returns -- Monthly (Reason: Standard Title (--))\n",
      "  Line 1201: Average Equal Weighted Returns -- Monthly (Reason: Standard Title (--))\n",
      "  Line 2387: Average Value Weighted Returns -- Annual (Reason: Standard Title (--))\n",
      "  Line 2489: Average Equal Weighted Returns -- Annual (Reason: Standard Title (--))\n",
      "  Line 2591: Number of Firms in Portfolios (Reason: Description Found Before Header on Line 2592)\n",
      "  Line 3777: Average Market Cap (Reason: Description Found Before Header on Line 3778)\n",
      "  Line 4964: Value Weight Average of BE/ME Calculated for June of t to June of t+1 as: (Reason: Description Found Before Header on Line 4968)\n",
      "  Line 6154: Value Weight Average of BE_FYt-1/ME_June t Calculated for June of t to June of t+1 as: (Reason: Description Found Before Header on Line 6158)\n",
      "  Line 7344: Value Weight Average of OP Calculated as: (Reason: Description Found Before Header on Line 7347)\n",
      "  Line 8089: Value Weight Average of investment (rate of growth of assets) Calculated as: (Reason: Description Found Before Header on Line 8092)\n",
      "\n",
      "Note: These are potential markers. Please review the file around these lines.\n",
      "The actual data usually starts on the line *after* the marker, or sometimes after the header line following the marker.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import io\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to your Fama-French data file\n",
    "file_path = \"../../Data/Fama_french/100_Portfolios_10x10.CSV\" # Using path provided by user\n",
    "\n",
    "# Keywords often found in header/marker lines preceding data blocks\n",
    "keywords = [\n",
    "    \"Average\", \"Returns\", \"Number of Firms\", \"Sum of\",\n",
    "    \"Value Weight\", \"Equal Weight\", \"Size\", \"BE/ME\",\n",
    "    \"Monthly\", \"Annual\", \"Portfolio\", \"Portfolios\"\n",
    "]\n",
    "# Minimum number of commas to guess if a line might be CSV data/header\n",
    "min_commas_for_header = 5 # Threshold for headers\n",
    "min_commas_for_data = 3   # Threshold for data\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def line_looks_like_data(line_stripped, min_commas):\n",
    "    \"\"\"Checks if a line looks like a typical data row (starts numeric, has commas).\"\"\"\n",
    "    if not line_stripped:\n",
    "        return False\n",
    "    parts = line_stripped.split(',')\n",
    "    if not parts: return False # Handle empty lines split\n",
    "    try:\n",
    "        float(parts[0].strip())\n",
    "        is_first_part_numeric = True\n",
    "    except ValueError:\n",
    "        is_first_part_numeric = False\n",
    "    return is_first_part_numeric and line_stripped.count(',') >= min_commas\n",
    "\n",
    "def line_looks_like_header(line_stripped, min_commas):\n",
    "    \"\"\"Checks if a line looks like a CSV header row (many commas, maybe non-numeric start).\"\"\"\n",
    "    if not line_stripped:\n",
    "        return False\n",
    "    # A header often has many commas\n",
    "    has_enough_commas = line_stripped.count(',') >= min_commas\n",
    "    # Optional: Check if it likely contains non-numeric characters typical of headers\n",
    "    has_text = bool(re.search(r'[a-zA-Z]', line_stripped))\n",
    "    # Optional: Check if it *doesn't* look like a standard data row\n",
    "    # is_data = line_looks_like_data(line_stripped, min_commas_for_data) # Avoid circularity if thresholds differ\n",
    "\n",
    "    # Primarily rely on comma count and presence of text\n",
    "    return has_enough_commas and has_text # and not is_data\n",
    "\n",
    "# --- Function to Identify Blocks ---\n",
    "def identify_potential_blocks(filepath, keywords_list, min_commas_hdr, min_commas_data):\n",
    "    \"\"\"Scans a file line by line to identify potential data block start points.\"\"\"\n",
    "    potential_blocks = []\n",
    "    lines = []\n",
    "    try:\n",
    "        with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 encoding failed, trying latin1...\")\n",
    "        with io.open(filepath, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return []\n",
    "\n",
    "    identified_lines = set() # Keep track of lines already marked as start points\n",
    "\n",
    "    # Iterate through lines, looking for potential markers\n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if not line_stripped or i in identified_lines:\n",
    "            continue\n",
    "\n",
    "        # --- Heuristic 1: Standard Title (keywords + '--') followed by data/header ---\n",
    "        contains_keywords = any(keyword.lower() in line_stripped.lower() for keyword in keywords_list)\n",
    "        looks_like_std_title = contains_keywords and \"--\" in line_stripped\n",
    "        next_line_looks_ok = False\n",
    "        if i + 1 < len(lines):\n",
    "            next_line_stripped = lines[i+1].strip()\n",
    "            if next_line_stripped:\n",
    "                 next_line_looks_ok = line_looks_like_data(next_line_stripped, min_commas_data) or \\\n",
    "                                      line_looks_like_header(next_line_stripped, min_commas_hdr)\n",
    "\n",
    "        if looks_like_std_title and next_line_looks_ok:\n",
    "            if i not in identified_lines:\n",
    "                 # Check if the previous identified block wasn't the immediately preceding line\n",
    "                 is_new_block = True\n",
    "                 if potential_blocks:\n",
    "                     last_block_line = potential_blocks[-1]['line_number']\n",
    "                     if i - last_block_line <= 2: # Allow for a blank line gap perhaps\n",
    "                         is_new_block = False\n",
    "\n",
    "                 if is_new_block:\n",
    "                    potential_blocks.append({\n",
    "                        \"line_number\": i,\n",
    "                        \"marker_text\": line_stripped,\n",
    "                        \"reason\": \"Standard Title (--)\"\n",
    "                    })\n",
    "                    identified_lines.add(i)\n",
    "                    continue # Prioritize this type of match\n",
    "\n",
    "        # --- Heuristic 2: Header line found, look backwards for description ---\n",
    "        is_header = line_looks_like_header(line_stripped, min_commas_hdr)\n",
    "        if is_header:\n",
    "            # Look back up to 5 lines for the first non-blank, non-data line containing keywords\n",
    "            found_description = False\n",
    "            for j in range(i - 1, max(-1, i - 6), -1): # Look back from i-1 down to i-5\n",
    "                if j < 0: break # Stop if we reach beginning of file\n",
    "                prev_line_stripped = lines[j].strip()\n",
    "\n",
    "                if not prev_line_stripped: # Skip blank lines\n",
    "                    continue\n",
    "\n",
    "                # Check if this previous line looks like a description\n",
    "                prev_contains_keywords = any(keyword.lower() in prev_line_stripped.lower() for keyword in keywords_list)\n",
    "                prev_is_data = line_looks_like_data(prev_line_stripped, min_commas_data)\n",
    "\n",
    "                if prev_contains_keywords and not prev_is_data:\n",
    "                    # Found a likely description line! Mark line j as the start.\n",
    "                    # Avoid adding if j is too close to the previously identified block start\n",
    "                    if j not in identified_lines:\n",
    "                        is_new_block = True\n",
    "                        if potential_blocks:\n",
    "                            last_block_line = potential_blocks[-1]['line_number']\n",
    "                            # Check if this found description line 'j' is sufficiently after the last block\n",
    "                            if j - last_block_line <= 5: # Heuristic: Allow some gap, but not too close\n",
    "                                # Also check if the header 'i' is very close to the last identified line\n",
    "                                if i - last_block_line <= 5:\n",
    "                                     is_new_block = False # Likely part of the previous block's header/desc\n",
    "\n",
    "                        if is_new_block:\n",
    "                            potential_blocks.append({\n",
    "                                \"line_number\": j, # Mark the description line\n",
    "                                \"marker_text\": prev_line_stripped,\n",
    "                                \"reason\": f\"Description Found Before Header on Line {i+1}\"\n",
    "                            })\n",
    "                            identified_lines.add(j)\n",
    "                            found_description = True\n",
    "                            break # Stop looking back once description found\n",
    "\n",
    "            # If we found a description by looking back, continue to next line 'i'\n",
    "            # This prevents the header line 'i' itself from being flagged by other heuristics later\n",
    "            if found_description:\n",
    "                 identified_lines.add(i) # Mark header line as processed too\n",
    "                 continue\n",
    "\n",
    "\n",
    "    # --- Post-processing: Sort results ---\n",
    "    potential_blocks.sort(key=lambda x: x['line_number'])\n",
    "\n",
    "    # Filter out blocks starting too close to each other (can happen with multi-line headers/titles)\n",
    "    final_blocks = []\n",
    "    last_line = -10 # Initialize far back\n",
    "    for block in potential_blocks:\n",
    "        if block['line_number'] - last_line > 2: # Only keep if sufficiently spaced from the last kept block\n",
    "             final_blocks.append(block)\n",
    "             last_line = block['line_number']\n",
    "        # else:\n",
    "            # print(f\"Filtering out block at line {block['line_number']+1} due to proximity to previous.\")\n",
    "\n",
    "\n",
    "    return final_blocks\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(f\"Scanning '{file_path}' for potential data block markers (v3)...\")\n",
    "blocks = identify_potential_blocks(file_path, keywords, min_commas_for_header, min_commas_for_data)\n",
    "\n",
    "if blocks:\n",
    "    print(\"\\nFound potential data block starting points:\")\n",
    "    for block in blocks:\n",
    "        # Add 1 to line_number for conventional 1-based line counting for display\n",
    "        print(f\"  Line {block['line_number'] + 1}: {block['marker_text']} (Reason: {block['reason']})\")\n",
    "    print(\"\\nNote: These are potential markers. Please review the file around these lines.\")\n",
    "    print(\"The actual data usually starts on the line *after* the marker, or sometimes after the header line following the marker.\")\n",
    "else:\n",
    "    print(\"Could not automatically identify distinct data blocks based on the v3 heuristics.\")\n",
    "    print(\"You may need to manually inspect the file to find the section headers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797ca61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading block 'VW_Returns_Monthly':\n",
      "  Marker Line (0-idx): 14\n",
      "  Header Line (0-idx): 15\n",
      "  Next Marker Line (0-idx): 1200\n",
      "  Calculated skip_rows: 15\n",
      "  Calculated n_rows: 1185\n",
      "  Successfully loaded and processed block 'VW_Returns_Monthly'. Shape: (1182, 101)\n",
      "Loading block 'EW_Returns_Monthly':\n",
      "  Marker Line (0-idx): 1200\n",
      "  Header Line (0-idx): 1201\n",
      "  Next Marker Line (0-idx): 2386\n",
      "  Calculated skip_rows: 1201\n",
      "  Calculated n_rows: 1185\n",
      "  Successfully loaded and processed block 'EW_Returns_Monthly'. Shape: (1182, 101)\n",
      "Loading block 'Num_Firms':\n",
      "  Marker Line (0-idx): 2590\n",
      "  Header Line (0-idx): 2591\n",
      "  Next Marker Line (0-idx): 3776\n",
      "  Calculated skip_rows: 2591\n",
      "  Calculated n_rows: 1185\n",
      "  Successfully loaded and processed block 'Num_Firms'. Shape: (1182, 101)\n",
      "Loading block 'Avg_Mkt_Cap':\n",
      "  Marker Line (0-idx): 3776\n",
      "  Header Line (0-idx): 3777\n",
      "  Next Marker Line (0-idx): 4963\n",
      "  Calculated skip_rows: 3777\n",
      "  Calculated n_rows: 1186\n",
      "  Successfully loaded and processed block 'Avg_Mkt_Cap'. Shape: (1182, 101)\n",
      "\n",
      "--- Loaded Data Summary ---\n",
      "\n",
      "--- VW_Returns_Monthly ---\n",
      "shape: (5, 101)\n",
      "┌────────────┬────────────┬─────────┬─────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
      "│ Date       ┆ SMALL LoBM ┆ ME1 BM2 ┆ ME1 BM3 ┆ … ┆ ME10 BM7 ┆ ME10 BM8 ┆ ME10 BM9 ┆ BIG HiBM │\n",
      "│ ---        ┆ ---        ┆ ---     ┆ ---     ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ date       ┆ f64        ┆ f64     ┆ f64     ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════════╪════════════╪═════════╪═════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
      "│ 1926-07-01 ┆ -99.99     ┆ 12.3656 ┆ -99.99  ┆ … ┆ 2.7332   ┆ 3.5356   ┆ 0.8576   ┆ -99.99   │\n",
      "│ 1926-08-01 ┆ -99.99     ┆ 2.9904  ┆ -99.99  ┆ … ┆ 6.7182   ┆ 3.237    ┆ 11.2245  ┆ -99.99   │\n",
      "│ 1926-09-01 ┆ -99.99     ┆ -18.583 ┆ -99.99  ┆ … ┆ -0.5241  ┆ -0.8665  ┆ -1.0703  ┆ -99.99   │\n",
      "│ 1926-10-01 ┆ -99.99     ┆ -4.1369 ┆ -99.99  ┆ … ┆ -5.5678  ┆ -1.8602  ┆ -3.9246  ┆ -99.99   │\n",
      "│ 1926-11-01 ┆ -99.99     ┆ -8.2589 ┆ -99.99  ┆ … ┆ 3.9514   ┆ 2.3695   ┆ 3.268    ┆ -99.99   │\n",
      "└────────────┴────────────┴─────────┴─────────┴───┴──────────┴──────────┴──────────┴──────────┘\n",
      "\n",
      "--- EW_Returns_Monthly ---\n",
      "shape: (5, 101)\n",
      "┌────────────┬────────────┬──────────┬─────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
      "│ Date       ┆ SMALL LoBM ┆ ME1 BM2  ┆ ME1 BM3 ┆ … ┆ ME10 BM7 ┆ ME10 BM8 ┆ ME10 BM9 ┆ BIG HiBM │\n",
      "│ ---        ┆ ---        ┆ ---      ┆ ---     ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ date       ┆ f64        ┆ f64      ┆ f64     ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════════╪════════════╪══════════╪═════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
      "│ 1926-07-01 ┆ -99.99     ┆ 11.3757  ┆ -99.99  ┆ … ┆ 3.4956   ┆ 4.0253   ┆ 0.8576   ┆ -99.99   │\n",
      "│ 1926-08-01 ┆ -99.99     ┆ 3.9643   ┆ -99.99  ┆ … ┆ 6.9075   ┆ 2.905    ┆ 11.2245  ┆ -99.99   │\n",
      "│ 1926-09-01 ┆ -99.99     ┆ -19.3658 ┆ -99.99  ┆ … ┆ -0.4491  ┆ -0.8531  ┆ -1.0703  ┆ -99.99   │\n",
      "│ 1926-10-01 ┆ -99.99     ┆ -4.9361  ┆ -99.99  ┆ … ┆ -4.4929  ┆ -1.9998  ┆ -3.9246  ┆ -99.99   │\n",
      "│ 1926-11-01 ┆ -99.99     ┆ -10.4167 ┆ -99.99  ┆ … ┆ 2.3656   ┆ 2.1261   ┆ 3.268    ┆ -99.99   │\n",
      "└────────────┴────────────┴──────────┴─────────┴───┴──────────┴──────────┴──────────┴──────────┘\n",
      "\n",
      "--- Num_Firms ---\n",
      "shape: (5, 101)\n",
      "┌────────────┬────────────┬─────────┬─────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
      "│ Date       ┆ SMALL LoBM ┆ ME1 BM2 ┆ ME1 BM3 ┆ … ┆ ME10 BM7 ┆ ME10 BM8 ┆ ME10 BM9 ┆ BIG HiBM │\n",
      "│ ---        ┆ ---        ┆ ---     ┆ ---     ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ date       ┆ f64        ┆ f64     ┆ f64     ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════════╪════════════╪═════════╪═════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
      "│ 1926-07-01 ┆ 0.0        ┆ 2.0     ┆ 0.0     ┆ … ┆ 5.0      ┆ 2.0      ┆ 1.0      ┆ 0.0      │\n",
      "│ 1926-08-01 ┆ 0.0        ┆ 2.0     ┆ 0.0     ┆ … ┆ 5.0      ┆ 2.0      ┆ 1.0      ┆ 0.0      │\n",
      "│ 1926-09-01 ┆ 0.0        ┆ 2.0     ┆ 0.0     ┆ … ┆ 5.0      ┆ 2.0      ┆ 1.0      ┆ 0.0      │\n",
      "│ 1926-10-01 ┆ 0.0        ┆ 2.0     ┆ 0.0     ┆ … ┆ 5.0      ┆ 2.0      ┆ 1.0      ┆ 0.0      │\n",
      "│ 1926-11-01 ┆ 0.0        ┆ 2.0     ┆ 0.0     ┆ … ┆ 5.0      ┆ 2.0      ┆ 1.0      ┆ 0.0      │\n",
      "└────────────┴────────────┴─────────┴─────────┴───┴──────────┴──────────┴──────────┴──────────┘\n",
      "\n",
      "--- Avg_Mkt_Cap ---\n",
      "shape: (5, 101)\n",
      "┌────────────┬────────────┬─────────┬─────────┬───┬──────────┬──────────┬──────────┬──────────┐\n",
      "│ Date       ┆ SMALL LoBM ┆ ME1 BM2 ┆ ME1 BM3 ┆ … ┆ ME10 BM7 ┆ ME10 BM8 ┆ ME10 BM9 ┆ BIG HiBM │\n",
      "│ ---        ┆ ---        ┆ ---     ┆ ---     ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---      │\n",
      "│ date       ┆ f64        ┆ f64     ┆ f64     ┆   ┆ f64      ┆ f64      ┆ f64      ┆ f64      │\n",
      "╞════════════╪════════════╪═════════╪═════════╪═══╪══════════╪══════════╪══════════╪══════════╡\n",
      "│ 1926-07-01 ┆ -99.99     ┆ 0.93    ┆ -99.99  ┆ … ┆ 297.42   ┆ 162.59   ┆ 180.73   ┆ -99.99   │\n",
      "│ 1926-08-01 ┆ -99.99     ┆ 1.05    ┆ -99.99  ┆ … ┆ 303.94   ┆ 167.22   ┆ 182.28   ┆ -99.99   │\n",
      "│ 1926-09-01 ┆ -99.99     ┆ 1.08    ┆ -99.99  ┆ … ┆ 322.14   ┆ 172.63   ┆ 202.74   ┆ -99.99   │\n",
      "│ 1926-10-01 ┆ -99.99     ┆ 0.88    ┆ -99.99  ┆ … ┆ 320.04   ┆ 171.13   ┆ 197.47   ┆ -99.99   │\n",
      "│ 1926-11-01 ┆ -99.99     ┆ 0.84    ┆ -99.99  ┆ … ┆ 300.95   ┆ 166.83   ┆ 189.72   ┆ -99.99   │\n",
      "└────────────┴────────────┴─────────┴─────────┴───┴──────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "file_path = '../../Data/Fama_french/100_Portfolios_10x10.CSV' # Using path provided by user\n",
    "missing_value = -99.99\n",
    "\n",
    "# Define the blocks to load based on the identifier script's output\n",
    "# Format: block_name: (marker_line_0_indexed, next_block_marker_line_0_indexed)\n",
    "# Use the line numbers where the *marker text* was found.\n",
    "# The header is assumed to be on the line *after* the marker line for standard titles,\n",
    "# or identified explicitly by the look-back heuristic.\n",
    "block_info = {\n",
    "    \"VW_Returns_Monthly\": (14, 1200), # Marker line 14 (\"Avg VW Returns -- Monthly\"), next marker line 1200 (\"Avg EW Returns -- Monthly\")\n",
    "    \"EW_Returns_Monthly\": (1200, 2386), # Marker line 1200, next marker line 2386 (\"Avg VW Returns -- Annual\")\n",
    "    \"Num_Firms\": (2590, 3776), # Marker line 2590 (\"Num Firms...\"), next marker line 3776 (\"Avg Mkt Cap...\")\n",
    "    \"Avg_Mkt_Cap\": (3776, 4963) # Marker line 3776, next marker line 4963 (\"VW Avg BE/ME...\")\n",
    "    # Note: Using line numbers (0-indexed) of the marker lines identified previously.\n",
    "    # The end line for the last block (Avg_Mkt_Cap) is the line before the next marker (4964-1 = 4963).\n",
    "}\n",
    "\n",
    "# --- Function to load and clean a specific block ---\n",
    "def load_fama_french_block(filepath, block_name, marker_line, next_marker_line, null_val):\n",
    "    \"\"\"Loads a specific block of Fama-French data using Polars.\"\"\"\n",
    "    try:\n",
    "        # Calculate skip_rows: Skip lines up to and including the marker line.\n",
    "        # The header is expected on the line immediately after the marker line.\n",
    "        # Example: If marker is line 14 (0-indexed), header is line 15. skip_rows should be 15.\n",
    "        rows_to_skip = marker_line + 1\n",
    "\n",
    "        # Calculate n_rows: Number of data rows between the current header and the next marker.\n",
    "        # header_line = marker_line + 1\n",
    "        # data_start_line = header_line + 1\n",
    "        # data_end_line = next_marker_line - 1\n",
    "        # n_rows = data_end_line - data_start_line + 1 = (next_marker_line - 1) - (marker_line + 1 + 1) + 1\n",
    "        # n_rows = next_marker_line - marker_line - 2\n",
    "        # However, polars n_rows reads *at most* n_rows *after* skipping rows *before* the header.\n",
    "        # If header is at rows_to_skip, data starts at rows_to_skip + 1.\n",
    "        # We want to read up to line next_marker_line - 1.\n",
    "        # Total lines in block = next_marker_line - rows_to_skip\n",
    "        # Number of data rows = Total lines in block - 1 (for header) = next_marker_line - rows_to_skip - 1\n",
    "        n_rows_to_read = next_marker_line - rows_to_skip\n",
    "\n",
    "        print(f\"Loading block '{block_name}':\")\n",
    "        print(f\"  Marker Line (0-idx): {marker_line}\")\n",
    "        print(f\"  Header Line (0-idx): {rows_to_skip}\")\n",
    "        print(f\"  Next Marker Line (0-idx): {next_marker_line}\")\n",
    "        print(f\"  Calculated skip_rows: {rows_to_skip}\")\n",
    "        print(f\"  Calculated n_rows: {n_rows_to_read}\")\n",
    "\n",
    "\n",
    "        df = pl.read_csv(\n",
    "            filepath,\n",
    "            skip_rows=rows_to_skip, # Skip lines *before* the header\n",
    "            n_rows=n_rows_to_read,  # Read this many rows *after* skipping\n",
    "            has_header=True,        # The first row read (after skipping) is the header\n",
    "            null_values=str(null_val),\n",
    "            separator=\",\",\n",
    "            ignore_errors=True,     # Try to ignore rows that cause parsing errors\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "\n",
    "        # --- Data Cleaning ---\n",
    "        # Rename the first column (date) - often unnamed or 'Unnamed: 0'\n",
    "        date_col_name = df.columns[0]\n",
    "        df = df.rename({date_col_name: \"Date\"})\n",
    "\n",
    "        # Convert Date column to string first to handle potential mixed types/whitespace\n",
    "        df = df.with_columns(pl.col(\"Date\").cast(pl.Utf8))\n",
    "\n",
    "        # Filter out rows where 'Date' is not numeric (removes potential footers/text)\n",
    "        df = df.filter(pl.col(\"Date\").is_not_null() & pl.col(\"Date\").str.contains(r\"^\\s*\\d+\\s*$\"))\n",
    "\n",
    "        # Convert Date column to actual Date type (YYYYMM format)\n",
    "        # Use strict=False to handle potential parsing errors gracefully (results in null)\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"Date\").str.strip_chars().str.strptime(pl.Date, format=\"%Y%m\", strict=False).alias(\"Date\")\n",
    "        )\n",
    "\n",
    "        # Convert all other columns to Float64 (suitable for returns, market cap, and can handle NaNs in Num Firms)\n",
    "        value_columns = df.columns[1:] # Exclude the 'Date' column\n",
    "        df = df.with_columns(\n",
    "            [pl.col(c).str.strip_chars().cast(pl.Float64, strict=False) for c in value_columns]\n",
    "        )\n",
    "\n",
    "        # Drop rows with invalid dates (NaT) or where all values are null\n",
    "        df = df.drop_nulls(subset=[\"Date\"])\n",
    "        df = df.filter(pl.sum_horizontal(pl.all().is_not_null()) > 1) # Keep rows with Date + at least one value\n",
    "\n",
    "        # Optional: Convert returns from percent to decimal (uncomment if needed)\n",
    "        # if \"Returns\" in block_name:\n",
    "        #     print(f\"  Converting returns in '{block_name}' to decimals...\")\n",
    "        #     for col_name in value_columns:\n",
    "        #         df = df.with_columns((pl.col(col_name) / 100.0).alias(col_name))\n",
    "\n",
    "        # Set Date as index (Polars doesn't have a direct index like pandas, but sorting helps)\n",
    "        df = df.sort(\"Date\")\n",
    "\n",
    "        print(f\"  Successfully loaded and processed block '{block_name}'. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    except pl.NoDataError:\n",
    "        print(f\"Warning: No data found for block '{block_name}'. Check markers and file structure.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred processing block '{block_name}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print detailed traceback for debugging\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "loaded_data = {}\n",
    "blocks_to_load = [\n",
    "    \"VW_Returns_Monthly\",\n",
    "    \"EW_Returns_Monthly\",\n",
    "    \"Num_Firms\",\n",
    "    \"Avg_Mkt_Cap\"\n",
    "]\n",
    "\n",
    "for block_name in blocks_to_load:\n",
    "    if block_name in block_info:\n",
    "        marker_line, next_marker_line = block_info[block_name]\n",
    "        try:\n",
    "            # Attempt to load using utf-8 first\n",
    "            df = load_fama_french_block(file_path, block_name, marker_line, next_marker_line, missing_value)\n",
    "            if df is None and isinstance(e, UnicodeDecodeError): # Check if loading failed due to encoding\n",
    "                 raise UnicodeDecodeError # Re-raise to trigger latin1 attempt\n",
    "            loaded_data[block_name] = df\n",
    "        except UnicodeDecodeError:\n",
    "             # If utf-8 fails, try latin1\n",
    "            print(f\"UTF-8 failed for {block_name}, trying latin1...\")\n",
    "            try:\n",
    "                 df = load_fama_french_block(file_path.replace('utf-8', 'latin1'), block_name, marker_line, next_marker_line, missing_value)\n",
    "                 loaded_data[block_name] = df\n",
    "            except Exception as e_latin1:\n",
    "                 print(f\"Latin1 encoding also failed for block '{block_name}': {e_latin1}\")\n",
    "                 loaded_data[block_name] = None\n",
    "        except Exception as e:\n",
    "             # Catch other potential errors during loading\n",
    "             print(f\"Failed to load block '{block_name}' due to error: {e}\")\n",
    "             loaded_data[block_name] = None # Ensure key exists even if loading fails\n",
    "    else:\n",
    "        print(f\"Warning: Block information for '{block_name}' not found.\")\n",
    "        loaded_data[block_name] = None\n",
    "\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Loaded Data Summary ---\")\n",
    "for block_name, df in loaded_data.items():\n",
    "    print(f\"\\n--- {block_name} ---\")\n",
    "    if df is not None:\n",
    "        print(df.head())\n",
    "        # print(df.describe()) # Uncomment for summary stats\n",
    "    else:\n",
    "        print(\"Failed to load.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
